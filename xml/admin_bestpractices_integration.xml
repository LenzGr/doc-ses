<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-bp-integration">
 <title>Integration</title>
 <sect1 xml:id="storage-bp-integration-kvm">
  <title>Storing &kvm; Disks in &ceph; Cluster</title>

  <para>
   You can create a disk image for &kvm;-driven virtual machine, store it in a
   &ceph; pool, optionally convert the content of an existing image to it, and
   then run the virtual machine with <command>qemu-kvm</command> making use of
   the disk image stored in the cluster. For more detailed information, see
   <xref linkend="cha-ceph-kvm"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-integration-libvirt">
  <title>Storing &libvirt; Disks in &ceph; Cluster</title>

  <para>
   Similar to &kvm; (see <xref linkend="storage-bp-integration-kvm"/>), you can
   use &ceph; to store virtual machines driven by &libvirt;. The advantage is
   that you can run any &libvirt;-supported virtualization solution, such as
   &kvm;, &xen;, or LXC. For more information, see
   <xref linkend="cha-ceph-libvirt"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-integration-xen">
  <title>Storing &xen; Disks in &ceph; Cluster</title>

  <para>
   One way to use &ceph; for storing &xen; disks is to make use of &libvirt; as
   described in <xref linkend="cha-ceph-libvirt"/>.
  </para>

  <para>
   Another option is to make &xen; talk to the <systemitem>rbd</systemitem>
   block device driver directly:
  </para>

  <procedure>
   <step>
    <para>
     If you have no disk image prepared for &xen;, create a new one:
    </para>
<screen>rbd create myimage --size 8000 --pool mypool</screen>
   </step>
   <step>
    <para>
     List images in the pool <literal>mypool</literal> and check if your new
     image is there:
    </para>
<screen>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Create a new block device by mapping the <literal>myimage</literal> image
     to the <systemitem>rbd</systemitem> kernel module:
    </para>
<screen>sudo rbd map --pool mypool myimage</screen>
    <tip>
     <title>User Name and Authentication</title>
     <para>
      To specify a user name, use <option>--id
      <replaceable>user-name</replaceable></option>. Moreover, if you use
      <systemitem>cephx</systemitem> authentication, you must also specify a
      secret. It may come from a keyring or a file containing the secret:
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
 /path/to/keyring</screen>
     <para>
      or
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     List all mapped devices:
    </para>
<screen>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
   </step>
   <step>
    <para>
     Now you can configure &xen; to use this device as a disk for running a
     virtual machine. You can for example add the following line to the
     <command>xl</command>-style domain configuration file:
    </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Mounting and Unmounting an RBD Image</title>

  <para>
   Images stored inside a &ceph; cluster pool can be mapped to a block device.
   You can then format such device, mount it to be able to exchange files, and
   unmount it when done.
  </para>

  <procedure>
   <step>
    <para>
     Make sure your &ceph; cluster includes a pool with the disk image you want
     to mount. Assume the pool is called <literal>mypool</literal> and the
     image is <literal>myimage</literal>.
    </para>
<screen>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Map the image to a new block device.
    </para>
<screen>sudo rbd map --pool mypool myimage</screen>
    <tip>
     <title>User Name and Authentication</title>
     <para>
      To specify a user name, use <option>--id
      <replaceable>user-name</replaceable></option>. Moreover, if you use
      <systemitem>cephx</systemitem> authentication, you must also specify a
      secret. It may come from a keyring or a file containing the secret:
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
 /path/to/keyring</screen>
     <para>
      or
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     List all mapped devices:
    </para>
<screen>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    <para>
     The device we want to work on is <filename>/dev/rbd0</filename>.
    </para>
   </step>
   <step>
    <para>
     Make an XFS file system on the <filename>/dev/rbd0</filename> device.
    </para>
<screen>sudo mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   </step>
   <step>
    <para>
     Mount the device and check it is correctly mounted. Replace
     <filename>/mnt</filename> with your mount point.
    </para>
<screen>sudo mount /dev/rbd0 /mnt
 mount | grep rbd0
 /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
    <para>
     Now you can move data from/to the device as if it was a local directory.
    </para>
    <tip>
     <title>Increasing the Size of RBD Device</title>
     <para>
      If you find that the size of the RBD device is no longer enough, you can
      easily increase it.
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Increase the size of the RBD image, for example up to 10GB.
       </para>
<screen>rbd resize --size 10000  mypool/myimage
 Resizing image: 100% complete...done.</screen>
      </listitem>
      <listitem>
       <para>
        Grow the file system to fill up the new size of the device.
       </para>
<screen>sudo xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
      </listitem>
     </orderedlist>
    </tip>
   </step>
   <step>
    <para>
     After you finish accessing the device, you can unmount it.
    </para>
<screen>sudo unmount /mnt</screen>
   </step>
   </procedure>
   <tip>
     <title>Manual (Un)mounting</title>
     <para>
      Since manually mapping/mounting RBD images after boot, and
      unmounting/unmapping them before shutdown can be tedious, a
      <command>rbdmap</command> script and &systemd; unit is provided. Refer to
      <command>man 8 rbdmap</command> for more details.
     </para>
    </tip>
 </sect1>
</chapter>
