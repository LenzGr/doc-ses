<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; from the previous
  release(s) to version &productnumber;.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package>,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Consider the following items before starting the upgrade procedure:
  </para>

  <variablelist>
   <varlistentry xml:id="upgrade.order">
    <term>Upgrade Order</term>
    <listitem>
     <para>
      Before upgrading the &ceph; cluster, you need to have both the underlying
      &sls; and &productname; correctly registered against SCC or SMT. You can
      upgrade daemons in your cluster while the cluster is online and in
      service. Certain types of daemons depend upon others. For example &ceph;
      &rgw;s depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
      upgrading in this order:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        &smaster;
       </para>
      </listitem>
      <listitem>
       <para>
        &mon;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mgr;s
       </para>
      </listitem>
      <listitem>
       <para>
        &osd;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mds;s
       </para>
      </listitem>
      <listitem>
       <para>
        &rgw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &igw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &ganesha;
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Delete Unnecessary Operating System Snapshots</term>
    <listitem>
     <para>
      Remove not needed file system snapshots on the operating system
      partitions of nodes. This ensures that there is enough free disk space
      during the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Check Cluster Health</term>
    <listitem>
     <para>
      Check the &ceph; cluster health before starting the upgrade procedure.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Upgrade One by One</term>
    <listitem>
     <para>
      Upgrade all the daemons of a specific type&mdash;for example all monitor
      daemons or all OSD daemons&mdash;one by one to ensure that they are all
      on the same release version. Also, upgrade all the daemons in your
      cluster before trying to exercise new functionality in a release.
     </para>
     <para>
      After all the daemons of a specific type are upgraded, check their
      status.
     </para>
     <para>
      Ensure that each monitor has rejoined the quorum after all monitors are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph mon stat</screen>
     <para>
      Ensure each &osd; daemon has rejoined the cluster after all OSDs are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph osd stat</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.upgrade.prev2curr">
  <title>Upgrade from &productname; &prevproductnumber; to &productnumber;</title>

  <para>
   This section provides details on upgrading &productname; from version
   &prevproductnumber; to &productnumber;.
  </para>

  <sect2 xml:id="upgrade.software_requirements">
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; &prevcephos;
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; &prevproductnumber;
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.consider_points">
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Because the migration of the underlying &prevcephos; to &cephos; cannot
      be done online, each node needs to be stopped for the duration of the OS
      upgrade. Therefore the services that the node provides will be
      unavailable for some time. The core cluster services will still be
      available&mdash;for example if one MON is down during upgrade, there are
      still at least two active MONs. Unfortunately, single instance services
      such as &igw;, &ogw; or &ganesha; will be unavailable.
     </para>
    </listitem>
    <listitem>
     <para>
      You need to shrink the &mds; (MDS) cluster. Because of incompatible
      features between the &productname; &prevproductnumber; and
      &productnumber; versions, the older MDS daemons will shutdown as soon as
      they see a single SES 6 level MDS join the cluster. Therefor it is
      necessary to shrink the MDS cluster to a single active MDS (and no
      standby's) for the duration of the MDS node upgrades. As soon as the
      second node is upgraded, you can extend the MDS cluster again.
     </para>
     <tip>
      <para>
       On a heavily loaded MDS cluster, you may need to to reduce the load (for
       example by stopping clients) so that a single active MDS is able to
       handle the workload.
      </para>
     </tip>
    </listitem>
    <listitem>
     <para>
      If &oa; is located on the &smaster;, it will be unavailable after you
      upgrade the node. The new &dashboard; will not be available until you
      deploy it by using &deepsea;.
     </para>
    </listitem>
    <listitem>
     <para>
      Before upgrading storage nodes, set the 'noout' flag which prevents
      &ceph; from rebalancing data during downtime and therefore avoids
      unnecessary data transfers:
     </para>
<screen>
&prompt.cephuser;ceph osd set noout
</screen>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a very long time&mdash;approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.one_node">
   <title>Single Node Upgrade</title>
   <para>
    You need to upgrade the cluster nodes one by one manually.
<!-- There are two ways you can perform the upgrade of a node: either using the installer DVD, or using the upgrade RPM package. -->
   </para>
   <sect3 xml:id="upgrade.one_node.manual">
    <title>Manual Node Upgrade using the Installer DVD</title>
    <procedure>
     <step>
      <para>
       Reboot the node from the &cephos; installer DVD/image.
      </para>
     </step>
     <step>
      <para>
       Select <guimenu>Upgrade</guimenu> from the boot menu.
      </para>
     </step>
     <step>
      <para>
       On the <guimenu>Previously Used Repositories</guimenu> screen, verify
       that the correct repositories are selected. If the system is not
       registered with SCC/SMT, you need to add the repositories manually. The
       list of corresponding repositories looks as follows:
      </para>
<screen>
SLE-Product-SLES/15-SP1
SLE-Module-Basesystem/15-SP1
SLE-Module-Server-Applications/15-SP1
Storage/6
</screen>
     </step>
     <step>
      <para>
       Review the <guimenu>Installation Settings</guimenu> and start the
       installation procedure by clicking <guimenu>Update</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
<!--
   <sect3 xml:id="upgrade.one_node.auto">
    <title>Automated Node Upgrade using the <emphasis>SLE15 Migration</emphasis> Package</title>
    <procedure>
     <step>
      <para>
       Disable or remove all existing software repositories:
      </para>
<screen>
&prompt.root;zypper mr -d -a
</screen>
     </step>
     <step>
      <para>
       Manually add the following software repositories:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         SLE-Product-SLES/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         SLE-Module-Basesystem/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         SLE-Module-Server-Applications/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         Storage/6
        </para>
       </listitem>
      </itemizedlist>
      <para>
       You can add them by finding out the right repository URL and running:
      </para>
<screen>
zypper ar -f <replaceable>REPO_URL</replaceable>
</screen>
      <para>
       for each required repository. Refresh them with <command>zypper
       ref</command>.
      </para>
     </step>
     <step>
      <para>
       Install the migration RPM packages. They adjust the &grub; boot loader
       to automatically trigger the upgrade on next reboot. Download the
       <package>SLES15-Migration</package> package from
       <link xlink:href="http://download.suse.de/ibs/home:/tserong/images/x86_64/"/>
       and the <package>suse-migration-activation</package> from
       <link xlink:href="http://download.suse.de/ibs/home:/tserong/SLE_12_SP3/noarch/"/>.
      </para>
     </step>
     <step>
      <para>
       Reboot. The migration system will start automatically and upgrade the
       packages by using the <command>zypper dup</command> command. The node
       will reboot automatically after the upgrade.
      </para>
      <tip>
       <title>Upgrade Failure</title>
       <para>
        If the upgrade fails, inspect
        <filename>/var/log/distro_migration.log</filename>. Then fix the
        problem, re-install the migration RPM packages, and reboot the node.
       </para>
      </tip>
     </step>
    </procedure>
   </sect3>
   -->
  </sect2>

  <sect2 xml:id="upgrade.procedure">
   <title>The Upgrade Procedure</title>
   <para>
    To upgrade the &productname; &prevproductnumber; cluster to version
    &productnumber;, follow instructions and procedures in this section.
   </para>
   <para>
    First upgrade the &smaster; (described in
    <xref linkend="upgrade.one_node" />). After the upgrade is complete, &oa;
    will no longer be installed. The following commands will still work,
    although &sminion;s are running old version of &ceph; and &salt;:
   </para>
<screen>
&prompt.cephuser;salt '*' test.ping
&prompt.cephuser;ceph status
</screen>
   <sect3>
    <title>Upgrade &mon;s</title>
    <procedure>
     <step>
      <para>
       Upgrade one &mon; node (described in
       <xref linkend="upgrade.one_node" />. The cluster will be running with
       mixed &ceph; versions.
      </para>
     </step>
     <step>
      <para>
       Upgrade the second &mon;.
      </para>
      <note>
       <title>Out of Quorum</title>
       <para>
        In your cluster contains exactly three &mon;s, during the second &mon;
        upgrade the two remaining &mon;s (the already upgraded one running
        &ceph; Nautilus, while the old one still running &ceph; Luminous) will
        be out of quorum. To fix it, run the following command on the &mon;
        node running &ceph; Luminous:
       </para>
<screen>
&prompt.cephuser;ceph daemon mon.<replaceable>LUMINOUS_MON_HOST_NAME</replaceable> quorum enter
</screen>
      </note>
     </step>
     <step>
      <para>
       Upgrade the rest of &mon;s, one by one.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Upgrade &osd;s</title>
    <tip>
     <para>
      Before upgrading the storage nodes, do not forget to run:
     </para>
<screen>
&prompt.cephuser;ceph osd set noout
</screen>
    </tip>
    <para>
     For each storage node, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Upgrade the OSD node (described in <xref linkend="upgrade.one_node" />.
      </para>
     </step>
     <step>
      <para>
       For each OSD's data partition, you need to find the correct
       <command>ceph-volume</command> command to activate it. Replace
       <replaceable>X1</replaceable> with the partition's correct
       letter/number:
      </para>
<screen>
 &prompt.cephuser;ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
      <para>
       For example:
      </para>
<screen>
&prompt.cephuser;ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     </step>
     <step>
      <para>
       The last line of the output contains the command to activate the
       partition:
      </para>
<screen>
&prompt.cephuser;ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--> All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--> Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     </step>
    </procedure>
    <tip>
     <para>
      After all OSD nodes are upgraded and their data partitions activated,
      run:
     </para>
<screen>
&prompt.cephuser;ceph osd unset noout
</screen>
    </tip>
    <para>
     Verify the cluster status. It will be similar to the following output:
    </para>
<screen>
&prompt.cephuser;ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
    <tip>
     <title>Check for the Version of Cluster Components/Nodes</title>
     <para>
      When you need to find out the versions of individual cluster components
      and nodes&mdash;for example to find out if all your nodes are actually on
      the same patch level after the upgrade&mdash;you can run
     </para>
<screen>&prompt.smaster;salt-run status.report</screen>
     <para>
      The command goes through the connected &sminion;s and scans for the
      version numbers of &ceph;, &salt;, and &sls;, and gives you a report
      displaying the version that the majority of nodes have and showing nodes
      whose version is different from the majority.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="upgrade.mds">
    <title>Upgrade &mds;'s</title>
    <procedure>
     <step>
      <para>
       Shrink the MDS cluster if you have more then 1 active MDS daemons, i.e.
       <option>max_mds</option> is &gt; 1. To shrink the MDS cluster, run
      </para>
<screen>
&prompt.cephuser;ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
      <para>
       where <replaceable>FS_NAME</replaceable> is the name of your &cephfs;
       instance ('cephfs' by default).
      </para>
     </step>
     <step>
      <para>
       Find the node hosting the MDS daemon that serves rank 0. Consult the
       output of the <command>ceph fs status</command> command and start the
       upgrade of the MDS cluster on this node.
      </para>
<screen>
&prompt.cephuser;ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
      <para>
       In this example, you need to start the upgrade procedure on node
       'mon1-6'.
      </para>
     </step>
     <step>
      <para>
       Upgrade the node following the procedure in
       <xref linkend="upgrade.one_node" />.
      </para>
     </step>
     <step>
      <para>
       Upgrade the remaining MDS nodes following the procedure in
       <xref linkend="upgrade.one_node" />.
      </para>
     </step>
     <step>
      <para>
       Reset <option>max_mds</option> to the desired configuration:
      </para>
<screen>
&prompt.smaster;ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Update <filename>policy.cfg</filename> and Deploy &dashboard; using &deepsea;</title>
    <procedure>
     <step>
      <para>
       On the &smaster;, edit
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and apply the
       following changes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Remove <literal>role-openattic</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Add <literal>role-storage</literal> for each storage node.
        </para>
       </listitem>
       <listitem>
        <para>
         Add <literal>role-prometheus</literal> and
         <literal>role-grafana</literal> to the &smaster; node.
        </para>
       </listitem>
       <listitem>
        <para>
         Remove obsolete <literal>profile-*</literal> lines.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Synchronize &deepsea; modules:
      </para>
<screen>
&prompt.smaster;salt '*' saltutil.sync_all
</screen>
     </step>
     <step>
      <para>
       Run &deepsea; stage 2 to update the roles:
      </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
</screen>
     </step>
     <step>
      <para>
       Cleanup &oa;:
      </para>
<screen>
&prompt.smaster;salt '*' state.apply ceph.rescind.openattic
&prompt.smaster;salt -I 'roles:master' state.apply ceph.remove.openattic
</screen>
     </step>
     <step>
      <para>
       Finally, run through &deepsea; stages 0-3:
      </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
     </step>
    </procedure>
    <para>
     After &deepsea; successfully finished Stage 3, the &dashboard; will be
     running. Refer to <xref linkend="ceph.dashboard" /> for a detailed
     overview of &dashboard; features.
    </para>
   </sect3>
   <sect3 xml:id="upgrade.drive_groups">
    <title>Migration from Profile-based Deployments to &drvgrps;</title>
    <para>
     In &productname; &prevproductnumber;, &deepsea; offered so called
     'profiles' to describe the layout of your OSDs. Starting with
     &productname; &productnumber;, we moved to a different approach called
     <emphasis>&drvgrps;</emphasis> (find more details in
     <xref linkend="ds.drive_groups" />).
    </para>
    <note>
     <para>
      Migrating to the new approach is not immediately mandatory. Destructive
      operations, such as <command>salt-run osd.remove</command>,
      <command>salt-run osd.replace</command>, or <command>salt-run
      osd.purge</command> are still available. However, adding new OSDs will
      require your action.
     </para>
    </note>
    <para>
     Because of the different approach of these implementations, we do not
     offer an automated migration path. However, we offer a variety of tools
     (in the form of &salt; runners) to make the migration as simple as
     possible.
    </para>
    <sect4>
     <title>Analyze the Current Layout</title>
     <para>
      To view information about the currently deployed OSDs, use the following
      command:
     </para>
<screen>
&prompt.smaster;salt-run disks.discover
</screen>
     <para>
      Alternatively, you can inspect the content of files in the
      <filename>/srv/pillar/ceph/proposals/profile-*/</filename> directories.
      The have similar structure to the following:
     </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
    </sect4>
    <sect4>
     <title>Create &drvgrps; Matching the Current Layout</title>
     <para>
      Refer to <xref linkend="ds.drive_groups.specs" /> for more details on
      &drvgrps; specification.
     </para>
     <para>
      The difference between a fresh deployment and upgrade scenario is that
      the drives to be migrated are already 'used'. Because
     </para>
<screen>
&prompt.smaster;salt-run disks.list
</screen>
     <para>
      looks for unused disks only, use
     </para>
<screen>
&prompt.smaster;salt-run disks.list include_unavailable=True
</screen>
     <para>
      Experiment with the &drvgrps; until you match your current setup. For a
      more visual representation of what will be happening, use the following
      command. Note that it has no output if there are no free disks:
     </para>
<screen>
&prompt.smaster;salt-run disks.report bypass_pillar=True
</screen>
     <para>
      If you verified that your &drvgrps; are properly configured and want to
      apply the new approach, remove files form the
      <filename>/srv/pillar/ceph/proposals/profile-*/</filename> directory and
      run &deepsea;'s stage.2 to refresh the &spillar;. Verify the result by
      running:
     </para>
<screen>
&prompt.smaster;salt target_node pillar.get ceph:storage
</screen>
     <warning>
      <title>Incorrect &drvgrps; Configuration</title>
      <para>
       If your &drvgrps; are not properly configured and there are spare disks
       in your setup, they will be deployed in the way you specified them. We
       recommend running:
      </para>
<screen>
&prompt.smaster;salt-run disks.report
</screen>
     </warning>
    </sect4>
    <sect4>
     <title>OSD Deployment</title>
     <para>
      For simple cases such as standalone OSDs, the migration will happen
      over-time. Whenever you remove or replace an OSD from the cluster, it
      will be replaced by a new, LVM based OSD.
     </para>
    </sect4>
    <sect4>
     <title>More Complex Setups</title>
     <para>
      If you have a more sophisticated setup than just sandalone OSDs, for
      example dedicated WAL/DBs or encrypted OSDs, the migration can only
      happen when all OSDs assigned to that WAL/DB device are removed. This is
      caused by the <command>ceph-volume</command> that creates Logical Volumes
      on disks before the deployment. This prevents the user from mixing
      partition based deployments with LV based deployments. In such cases it
      is best to manually remove all OSDs that are assigned to a WAL/DB device
      and re-deploy them using the &drvgrps; approach.
     </para>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
</chapter>
